\documentclass[twocolumn]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\newcommand{\myimage}[3]{
\begin{figure}[!ht]
\caption{#2}
\label{#3}
\includegraphics[width=0.4\textwidth]{#1}
\end{figure}
}
\title{Interactive Parallel Parabola Flow}
\author{
Micah Corah
\and
Dan Ibanez
\and
David McWilliams
\and
Han Wang
}
\begin{document}
\maketitle
\section{Introduction}
\section{Physics}
%David
\section{Parallel Operation and Optimization}
%Han
The whole calculation of this code is a iteration procedure.
Matrix Psi is the matrix of streamlines and matrix Omega is the matrix of vortices.
Matrix Psi is calculated in function PsiCalc with an iterative method.
Function PsiCalc is the most time consuming function in the serial version
of this code.
It may need several or tens of iterations to finish.
As a finite difference problem, new Psi and Omega matrices are calculated from
the old Psi and Omega matrices.
Every elements in new Psi and omega matrix is a function of 5 elements in the old
matrix.
\[\Psi_{i,j}=f(\Psi^0_{i,j},\Psi^0_{i-1,j},\Psi^0_{i+1,j},\Psi^0_{i,j+1},\Psi^0_{i,j-1})\]
%\[New\_Psi[i][j]=
%Function( Old\_Psi[i][j],
%          Old\_Psi[i-1][j],
%          Old\_Psi[i+1][j],
%          Old\_Psi[i][j+1],
%          Old\_Psi[i][j-1] )\]
For the parallelization scheme, a processor only stores and calculates a part
of the Psi and Omega matrix.
The Psi and omega are all (Nx+2)*(My+2) matrix.
They both have two rows at the top and bottom and two columns at the right
and left that considered as boundary, which are set in the BCs function.
We use a highly symmetric configuration for parallelization.
Every processors calculate and store a (nx+2)*(my+2) matrix.
They also have  two rows at the top and bottom and two columns at the right
and left that are considered as boundary.
And they do not calculate the boundary.
Small matrices  overlap with the adjacent matrix with two rows or two columns.
The boundary of each processors are calculated by other processors.
 
\begin{figure}[H]
\begin{center}
\includegraphics[width=10cm]{configuration.png}
\caption{Configuration of Processors}
\label{fig:configuration}
\end{center}
\end{figure}

Figure  \ref{fig:configuration} is an example.
In figure \ref{fig:configuration}, "o" is an element in the Psi or Omega matrix.
Row 1,6 and column 1,8 are the boundaries.
Here, we have 4 processors.
Processor 1 stores the part covered by red square
which ranges from row 1 to 5 and column 1 to 5.
Processor 2 stores the part covered by green square,which ranges from
row 1 to 5 and from column 4 to 8.
Processor 3 stores the elements covered by the blue square.
But processor 1 only calculate elements from row 2 to row 4 and from column
2 to column 4.
The rest elements are considered as the boundary of processor 1.
Although column 5 is the boundary of processor 1,  it is not the boundary
of processor 2.
So processor 2 will calculate column 5 and send the value of these elements
to processor 1. In the same way, processor 3 will calculate row 5 and send
them to processor 1.
So processor 1 will have updated value for all the elements.
For the same reason, all the processors will get the new values for the matrices
after one iteration.
The criterion to stop the iteration is the maximum difference between
the old matrix and the new matrix is smaller than a specific value PsiTol.
So when finishing an iteration, the maximum difference between the old Psi
matrix and the new Psi matrix is calculated in all the processor.
We use \texttt{MPI\_Allreduce} to gather the maximum value in different
processors and compare them with PsiTol to determine whether to continue
next iteration or not.
 
\section{Parallel Boundary Conditions}
%Micah
\section{Parallel File IO}
%Dan
In order to easily verify our results using the serial code,
we chose to implement single-file IO.
On the other hand, we would like to get some parallelism in
file IO so that it does not cripple scaling, so we try to use
MPI's parallel file IO routines to write the file in parallel.
An additional complication is that the file is in a text format
rather than binary.
Fortunately, the file is formatted such that all values are
padded to a known character width, so the offset in characters
of every value and the total file size are easily computable.
To write our output file, we first open a file collectively
with \texttt{MPI\_File\_open} and then 
allocate enough characters for the whole output with
\texttt{MPI\_File\_preallocate}.
We use
\texttt{sprintf} to produce a text string of know length for
each floating-point value, and then use
\texttt{MPI\_File\_write\_at} to write the text string at
the correct location in the file.
Each process computes its own offsets and writes concurrently
without any synchronization until all processes close the file.
This is theoretically a scalable algorithm, although in practice
operating system and MPI limits are likely to limit scaling
to some upper bound.
\section{Results}
\section{Future Work}
\section{Conclusions}
\section{References}

\end{document}
